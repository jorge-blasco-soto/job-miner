# JobMiner - Project Summary

## ğŸ¯ What You Got

A **complete, production-ready job scraper system** that automatically finds and filters senior tech jobs at established companies, running entirely for free using GitHub Actions and local LLM.

## ğŸ“¦ What's Included

### Core Application
- âœ… **Python package** (`jobminer/`) with Poetry dependency management
- âœ… **Multi-source scraping**: RemoteOK, WeWorkRemotely, Remotive, company career pages
- âœ… **LLM filtering**: Local Ollama integration (llama2) to rank job relevance
- âœ… **Company database**: 80+ pre-vetted companies (200+ employees, 5+ years, stable)
- âœ… **Smart deduplication**: Merges results across runs
- âœ… **Multiple outputs**: JSON + CSV formats

### Automation
- âœ… **GitHub Actions workflow**: Runs daily at 9 AM UTC
- âœ… **Auto-commit results**: Updates data files automatically
- âœ… **Job summaries**: Visual reports in GitHub Actions UI
- âœ… **Manual trigger**: Can run on-demand

### Docker Support
- âœ… **Dockerfile**: Single-container setup
- âœ… **Docker Compose**: Multi-service with Ollama
- âœ… **Local testing**: Easy development environment

### Documentation
- âœ… **README.md**: Comprehensive guide (features, setup, config)
- âœ… **QUICKSTART.md**: 5-minute setup guide
- âœ… **CONTRIBUTING.md**: Developer guidelines
- âœ… **Issue templates**: For bugs, features, company additions

### Utilities
- âœ… **setup.sh**: Automated setup script
- âœ… **Makefile**: Convenient commands (run, clean, view, etc.)
- âœ… **analyze_jobs.py**: Stats and visualization tool
- âœ… **.env.example**: Configuration template

## ğŸª Target Jobs

**Roles:**
- Data Engineer
- Senior Data Engineer
- Software Engineer
- Solutions Architect

**Companies (Examples):**
- Cloud/Data: Snowflake, Databricks, MongoDB, Confluent, Elastic
- Big Tech: Microsoft, Amazon, Google, Meta, Netflix
- SaaS: Salesforce, ServiceNow, Workday, Atlassian
- FinTech: Stripe, Square, PayPal
- Security: CrowdStrike, Palo Alto Networks, Okta
- **80+ total companies**

**Requirements:**
- âœ… Remote positions
- âœ… 200+ employees
- âœ… 5+ years in business
- âœ… Stable growth
- âœ… Preference for public companies

## ğŸš€ How to Use

### Option 1: GitHub Actions (Zero Maintenance)
```bash
1. Fork repo
2. Enable Actions with write permissions
3. Push to trigger
â†’ Jobs auto-scraped daily, results committed to repo
```

### Option 2: Local Quick Run
```bash
./setup.sh          # One-time setup
ollama serve &      # Start LLM
poetry run jobminer # Run scraper
```

### Option 3: Docker
```bash
docker-compose up   # Everything included
```

## ğŸ“Š What You Get

After each run:
```
data/
â”œâ”€â”€ jobs_latest.json          # All jobs (with scores)
â”œâ”€â”€ jobs_latest.csv           # Excel-friendly format
â”œâ”€â”€ jobs_YYYYMMDD_HHMMSS.json # Historical snapshots
â””â”€â”€ result_*.json             # Run metadata
```

**Job Data Structure:**
```json
{
  "title": "Senior Data Engineer",
  "company": "Snowflake",
  "url": "https://...",
  "location": "Remote",
  "is_remote": true,
  "relevance_score": 0.95,
  "llm_analysis": "Excellent match because...",
  "scraped_at": "2024-10-31T12:00:00"
}
```

## ğŸ”§ Key Features

### Smart Filtering
1. **Company Filter**: Only from your curated 80+ companies
2. **Role Filter**: Matches your target positions
3. **Remote Filter**: Remote-only if configured
4. **LLM Ranking**: 0.0-1.0 score based on your criteria
5. **Threshold**: Only saves jobs with score â‰¥ 0.5

### Data Quality
- Deduplication by URL
- Merges with historical data
- Keeps best scoring jobs
- Tracks when scraped

### Automation
- Runs on schedule (daily 9 AM UTC)
- Manual trigger available
- Auto-commits results
- Creates job summary reports
- Artifact upload for history

## ğŸ’° Cost

**$0.00 / month**

Free tools used:
- GitHub Actions (2,000 min/month free)
- Ollama (local LLM, free)
- All scraping sources (free APIs)
- Docker (free)
- Poetry (free)

## ğŸ“ˆ Expected Results

**First run:**
- ~50-100 jobs found
- ~20-30 after LLM filtering
- Takes 5-10 minutes

**Daily runs:**
- New jobs added
- Deduplicated
- Historical data preserved

## ğŸ›ï¸ Customization

Everything is configurable via `.env`:

```bash
# Change target roles
TARGET_ROLES=your,desired,roles

# Adjust company criteria
MIN_EMPLOYEES=300
MIN_YEARS_IN_BUSINESS=10

# LLM settings
OLLAMA_MODEL=mistral  # or codellama, llama2

# Output preferences
MAX_JOBS_PER_RUN=200
OUTPUT_FORMAT=json,csv
```

## ğŸ› ï¸ Maintenance

**To add companies:**
Edit `jobminer/companies.py`

**To add job boards:**
Add scraper to `jobminer/scrapers/job_boards.py`

**To improve filtering:**
Edit prompts in `jobminer/llm_filter.py`

## ğŸ“ File Structure

```
jobminer/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/scrape.yml      # GitHub Actions
â”‚   â””â”€â”€ ISSUE_TEMPLATE/           # Issue templates
â”œâ”€â”€ jobminer/                     # Main package
â”‚   â”œâ”€â”€ main.py                   # Entry point
â”‚   â”œâ”€â”€ config.py                 # Settings
â”‚   â”œâ”€â”€ models.py                 # Data models
â”‚   â”œâ”€â”€ companies.py              # Company database â­
â”‚   â”œâ”€â”€ llm_filter.py             # LLM integration
â”‚   â”œâ”€â”€ scraper.py                # Orchestration
â”‚   â””â”€â”€ scrapers/
â”‚       â”œâ”€â”€ base.py               # Base class
â”‚       â””â”€â”€ job_boards.py         # Scrapers â­
â”œâ”€â”€ data/                         # Output directory
â”œâ”€â”€ pyproject.toml                # Dependencies
â”œâ”€â”€ Dockerfile                    # Docker image
â”œâ”€â”€ docker-compose.yml            # Multi-container
â”œâ”€â”€ setup.sh                      # Setup script
â”œâ”€â”€ Makefile                      # Commands
â”œâ”€â”€ analyze_jobs.py               # Analysis tool
â”œâ”€â”€ .env.example                  # Config template
â”œâ”€â”€ README.md                     # Full docs
â”œâ”€â”€ QUICKSTART.md                 # Quick guide
â””â”€â”€ CONTRIBUTING.md               # Dev guide
```

## ğŸ‰ Next Steps

1. **Initial Setup:**
   ```bash
   cd jobminer
   ./setup.sh
   ```

2. **Local Test:**
   ```bash
   ollama serve &
   poetry run jobminer
   ./analyze_jobs.py
   ```

3. **View Results:**
   ```bash
   make view-latest
   # or
   open data/jobs_latest.csv
   ```

4. **GitHub Automation:**
   - Create GitHub repo
   - Push code
   - Enable Actions
   - Let it run automatically!

5. **Customize:**
   - Edit `.env` for preferences
   - Add companies to database
   - Adjust LLM prompts

## ğŸ†˜ Need Help?

- **Docs**: `README.md` (comprehensive)
- **Quick Start**: `QUICKSTART.md` (5-min guide)
- **Commands**: `make help`
- **Analysis**: `./analyze_jobs.py --help`

## ğŸ Bonus Features

- **Makefile commands**: `make view-latest`, `make check-ollama`
- **Analysis tool**: Stats, visualizations, export top jobs
- **GitHub Actions summary**: Visual job reports
- **Historical tracking**: Never lose a job posting
- **CSV export**: Easy Excel/Sheets import

## ğŸŒŸ You Now Have

âœ… Automated job discovery
âœ… AI-powered relevance filtering
âœ… 80+ vetted companies
âœ… Daily automated runs
âœ… Zero ongoing costs
âœ… Complete customization
âœ… Professional codebase
âœ… Full documentation

---

**Everything you need to find your next senior tech role!** ğŸš€

Start with: `./setup.sh` then `poetry run jobminer`

Or push to GitHub and let Actions handle everything!
